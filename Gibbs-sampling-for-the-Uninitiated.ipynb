{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import beta, binomial, dirichlet, multinomial\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize(V, N, labels, a=1.0, b=1.0, eta=1.0):\n",
    "    pi = beta(a, b)\n",
    "    L = binomial(1, pi, N)\n",
    "    for n in range(N):\n",
    "        if labels[n] == -1: continue\n",
    "        L[n] = labels[n]\n",
    "    theta_0, theta_1 = dirichlet([eta] * V, 2)\n",
    "    return theta_0, theta_1, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(V, N, M=100, a=1.0, b=1.0, eta=1.0):\n",
    "  theta_0, theta_1, labels = initialize(V, N, [-1] * N, a=a, b=b, eta=eta)\n",
    "  corpus = []\n",
    "  for n in range(N):\n",
    "    theta = theta_0 if labels[n] == 0 else theta_1\n",
    "    corpus.append(multinomial(M, theta))\n",
    "  return corpus, labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update(theta_0, theta_1, L, corpus, labels, a=1.0, b=1.0, eta=1.0):\n",
    "\n",
    "    N = len(corpus)\n",
    "    V = len(corpus[0])\n",
    "    \n",
    "    C = [len([l for l in L if l == 0]), len([l for l in L if l == 1])]\n",
    "    \n",
    "    # compute current words counts:\n",
    "    cnts = []\n",
    "    for v in range(V):\n",
    "        word_cnts = [a, b]\n",
    "        for n in range(N):\n",
    "            word_cnts[L[n]] += corpus[n][v]\n",
    "        cnts.append(word_cnts)\n",
    "    \n",
    "    # update document labels\n",
    "    for n in range(N):\n",
    "        l = L[n]\n",
    "        if labels[n] == -1:\n",
    "            # subtract document's word counts from the corresponding class counts:\n",
    "            for v in range(V):\n",
    "                cnts[v][l] -= corpus[n][v]\n",
    "            # subtract 1 from class counts for the corresponding label:\n",
    "            C[l] -= 1\n",
    "            # compute document label probabilities\n",
    "            if C[0] == 0: p = 1.0\n",
    "            elif C[1] == 0: p = 0.0\n",
    "            else:\n",
    "              # compute the product of probabilities (sum of logs)\n",
    "              value0 = np.log(float(C[0])/(N+a+b-1))\n",
    "              value1 = np.log(float(C[1])/(N+a+b-1))\n",
    "              for v in range(V):\n",
    "                  value0 += np.log(theta_0[v])*corpus[n][v]\n",
    "                  value1 += np.log(theta_1[v])*corpus[n][v]\n",
    "              # Divide by a common number to avoid problems of small numbers.\n",
    "              divisor = (value0+value1)/2\n",
    "              value0 = np.exp(value0-divisor)\n",
    "              value1 = np.exp(value1-divisor)\n",
    "              p = value1 / (value0 + value1)\n",
    "            # sample the new label:\n",
    "            L[n] = binomial(1, p)\n",
    "            # update document count for this label\n",
    "            C[L[n]] += 1\n",
    "            # add document's word counts from the corresponding class counts:\n",
    "            for v in range(V): cnts[v][L[n]] += corpus[n][v]\n",
    "      \n",
    "    # sample new theta\n",
    "    t0 = []\n",
    "    t1 = []\n",
    "    for v in range(V):\n",
    "        t0.append(cnts[v][0])\n",
    "        t1.append(cnts[v][1])\n",
    "    theta_0 = dirichlet(t0)\n",
    "    theta_1 = dirichlet(t1)\n",
    "  \n",
    "    return theta_0, theta_1, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(corpus, labels, iterations=100, a=1.0, b=1.0, eta=1.0):\n",
    "    N = len(corpus)\n",
    "    V = len(corpus[0])\n",
    "    theta_0, theta_1, L = initialize(V, N, labels, a=a, b=b, eta=eta)\n",
    "    trace = [L]\n",
    "    \n",
    "    for t in range(iterations):\n",
    "        theta_0, theta_1, L = update(theta_0, theta_1, L, corpus, labels, a=a, b=b, eta=eta)\n",
    "        trace.append(L)\n",
    "    \n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(N, p, L, true_labels):\n",
    "    predicted_labels = [L[i] for i in range(int(N * p), N)] \n",
    "    correct = 0\n",
    "    for i in range(0,len(predicted_labels)):\n",
    "        if predicted_labels[i] == true_labels[i]: correct += 1\n",
    "    accuracy = float(correct)/len(predicted_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_simulation(N=50, V=5, M=100, iterations=100, p=0.7, a=2, b=2, eta=1.0):\n",
    "    corpus, labels = generate_data(V, N, M, a=a, b=b, eta=eta)\n",
    "    # Partition the data into training and test sets:\n",
    "    true_labels = []\n",
    "    for i in range(int(N * p), N):\n",
    "        true_labels.append(labels[i])\n",
    "        labels[i] = -1\n",
    "    trace = sample(corpus, labels, iterations=iterations, a=a, b=b, eta=eta)\n",
    "    return compute_accuracy(N, p, trace[-1], true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 1.000\n",
      "Average accuracy = 1.000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncZekBhl3SWqQcZekBhl3SWqQcZekBv0f6rN9xwRZUvkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f90242f3390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "cnt = 0\n",
    "while cnt < 1:\n",
    "    accuracy = run_simulation(N=500, V=500, M=100, iterations=100, p=0.75, a=2, b=2, eta=1.0)\n",
    "    results.append(accuracy)\n",
    "    cnt += 1\n",
    "print(\"Results: %s\" % \", \".join(\"%0.3f\" % x for x in results))\n",
    "print(\"Average accuracy = %0.3f\" % np.average(results))\n",
    "plt.boxplot(results);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.9 s, sys: 8 ms, total: 43.9 s\n",
      "Wall time: 43.9 s\n"
     ]
    }
   ],
   "source": [
    "%time accuracy = run_simulation(N=500, V=1000, M=100, iterations=100, p=0.75, a=2, b=2, eta=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = [] \n",
    "for i in range(int(N * p),N): results.append(traces[-1][i])\n",
    "correct = 0\n",
    "for i in range(0,len(results)):\n",
    "    if results[i] == target[i]: correct += 1\n",
    "accuracy = float(correct)/len(results)\n",
    "print(\"Accuracy is\", accuracy)\n",
    "plt.boxplot(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t, p in zip(target, traces[-1][int(N * p):]): print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = genNB(n,m)\n",
    "      accuracy.append(gibbsSampling(data['data'],data['l'],iterations,partition))\n",
    "    print(\"Accuracy average at {0} is {1}\".format(partition,numpy.average(accuracy)))\n",
    "sample(corpus, labels, iterations=100, a=1.0, b=1.0, eta=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('a -->>', state['a'])\n",
    "print('b -->>', state['b'])\n",
    "print('eta -->>', state['eta'])\n",
    "print('L -->>', state['L'][:10])\n",
    "print('theta-->>', state['theta'])\n",
    "print('corpus-->>', state['corpus'])\n",
    "print(state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_labels(): pass\n",
    "def update_theta(): pass\n",
    "def update(V, N, training_label_map, L, theta_0, theta_1): \n",
    "    # document counts:\n",
    "    C_0 = len([l for l in L if l == 0])\n",
    "    C_1 = N - C_0\n",
    "    word_cnts = []\n",
    "      # Counts of words\n",
    "    for i in range(V):\n",
    "    nc = [1,1]\n",
    "    for j in range(0,n):\n",
    "      nc[prevsample['l'][j]] += prevsample['data'][j][i]\n",
    "    NC.append(nc)\n",
    "\n",
    "  \n",
    "def gibbsIterate(prevsample,labels):\n",
    "  \"Gibbs sampler iteration\"\n",
    "  n = len(prevsample['data'])\n",
    "  m = len(prevsample['data'][0])\n",
    "  \n",
    "  # Counts of documents\n",
    "  C = [len([l for l in prevsample['l'] if l==0]),\n",
    "       len([l for l in prevsample['l'] if l==1])]\n",
    "       \n",
    "  # Counts of words\n",
    "  NC = list()\n",
    "  for i in range(0,m):\n",
    "    nc = [1,1]\n",
    "    for j in range(0,n):\n",
    "      nc[prevsample['l'][j]] += prevsample['data'][j][i]\n",
    "    NC.append(nc)\n",
    "    \n",
    "  for j in range(0,n):\n",
    "    # for each document label\n",
    "    if labels[j] == -1:\n",
    "      # If the label is not from the training set, let's resample it\n",
    "      \n",
    "      # subtract j's word counts from NC of whatever class it's member of\n",
    "      for i in range(0,m):\n",
    "        NC[i][prevsample['l'][j]] -= prevsample['data'][j][i]\n",
    "\t\n",
    "      # subtract 1 from the count of documents with label Lj\n",
    "      C[prevsample['l'][j]] -= 1\n",
    "      \n",
    "      if C[0] == 0:\n",
    "        p1 = 1\n",
    "      elif C[1] == 0:\n",
    "        p1 = 0\n",
    "      else:\n",
    "          # compute the product of probabilities (sum of logs)\n",
    "          value0 = log(float(C[0])/(n+1))\n",
    "          value1 = log(float(C[1])/(n+1))\n",
    "          for i in range(0,m):\n",
    "              value0 += log(prevsample['theta'][0][i])*prevsample['data'][j][i]\n",
    "              value1 += log(prevsample['theta'][1][i])*prevsample['data'][j][i]\n",
    "          # Divide by a common number to avoid problems of small numbers.\n",
    "          divisor = (value0+value1)/2\n",
    "          value0 = exp(value0-divisor)\n",
    "          value1 = exp(value1-divisor)\n",
    "          p1 = value1 / (value0 + value1)\n",
    "      \n",
    "      # sample the new label\n",
    "      prevsample['l'][j] = mtrand.binomial(1,p1)\n",
    "      \n",
    "      # add 1 to the counts of documents with label Lj\n",
    "      C[prevsample['l'][j]] += 1\n",
    "      \n",
    "      # add j's word counts to the total word counts for class Lj\n",
    "      for i in range(0,m):\n",
    "        NC[i][prevsample['l'][j]] += prevsample['data'][j][i]\n",
    "      \n",
    "  # sample new theta\n",
    "  t0 = list()\n",
    "  t1 = list()\n",
    "  for i in range(0,m):\n",
    "    # For each word\n",
    "    t0.append(NC[i][0])\n",
    "    t1.append(NC[i][1])\n",
    "  prevsample['theta'][0] = mtrand.dirichlet(t0)\n",
    "  prevsample['theta'][1] = mtrand.dirichlet(t1)\n",
    "  \n",
    "  return prevsample\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from numpy.random import mtrand\n",
    "from numpy import log, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.random.beta(a, b, size=None)\n",
    "numpy.random.dirichlet(alpha, size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nbgibbs.py - An implementation of the Naive Bayes Gibbs sampler described in\n",
    "# Resnik & Hardisty (2010), Gibbs sampling for the uninitiated, CS-TR-4956\n",
    "# Author: Diego Molla <dmollaaliod@gmail.com>\n",
    "# Date: 30 August 2011\n",
    "# http://sourceforge.net/p/nbgibbs\n",
    "\n",
    "from numpy.random import mtrand\n",
    "from numpy import log, exp\n",
    "\n",
    "def initialise(n,m,labels):\n",
    "  # Generate the probability of each of the m words in each class\n",
    "  theta = list()\n",
    "  gamma_theta = [1]*m\n",
    "  theta.append(mtrand.dirichlet(gamma_theta))\n",
    "  theta.append(mtrand.dirichlet(gamma_theta))\n",
    "  # Generate the labels\n",
    "  pi = mtrand.beta(1,1)\n",
    "  l = mtrand.binomial(1,pi,size=n)\n",
    "  # Overwrite with training data\n",
    "  for i in range(0,n):\n",
    "    if labels[i] != -1:\n",
    "      l[i] = labels[i]\n",
    "  # Return results\n",
    "  return {'l':l,'theta':theta,'pi':pi}\n",
    "\n",
    "def genNB(n,m,w=100):\n",
    "  \"\"\"Generate a sample for testing\n",
    "  n documents\n",
    "  m distinct words\n",
    "  w words per document\"\"\"\n",
    "  d = initialise(n,m,[-1]*n)\n",
    "  data = list()\n",
    "  for i in range(0,n):\n",
    "    label = d['l'][i]\n",
    "    data.append(mtrand.multinomial(w,d['theta'][label]))\n",
    "  d['data'] = data\n",
    "  return d\n",
    "  \n",
    "def gibbsSampling(observations,labels,iterations,partition=0.7):\n",
    "  \"Run NB Gibbs sampling\"\n",
    "  n = len(observations)\n",
    "  m = len(observations[0])\n",
    "  \n",
    "  #  print \"Target labels are\", labels\n",
    "  \n",
    "  # Partition the data set: 70% train, 30% test\n",
    "  target = list()\n",
    "  for i in range(int(n*partition),n):\n",
    "    target.append(labels[i])\n",
    "    labels[i] = -1\n",
    "\n",
    "  s = initialise(n,m,labels)\n",
    "  #  print \"Target theta is\", s['theta']\n",
    "  s['data'] = observations\n",
    "  result = [s['l']]\n",
    "  #  print s['l']\n",
    "  for t in range(0,iterations):\n",
    "    s = gibbsIterate(s,labels)\n",
    "    result.append(s['l'])\n",
    "  #    print s['l']\n",
    "  #    print \"Theta is\", s['theta']\n",
    "\n",
    "  # Evaluation of accuracy. We take the result of the last iteration.\n",
    "  # We would probably have better results if we took the mode of\n",
    "  # all iterations.\n",
    "  results = list()\n",
    "  for i in range(int(n*partition),n):\n",
    "    results.append(s['l'][i])\n",
    "  correct = 0\n",
    "  for i in range(0,len(results)):\n",
    "    if results[i] == target[i]:\n",
    "      correct += 1\n",
    "  accuracy = float(correct)/len(results)\n",
    "  #  print \"Accuracy is\", accuracy\n",
    "  return accuracy\n",
    "  #  return result\n",
    "  \n",
    "def gibbsIterate(prevsample,labels):\n",
    "  \"Gibbs sampler iteration\"\n",
    "  n = len(prevsample['data'])\n",
    "  m = len(prevsample['data'][0])\n",
    "  \n",
    "  # Counts of documents\n",
    "  C = [len([l for l in prevsample['l'] if l==0]),\n",
    "       len([l for l in prevsample['l'] if l==1])]\n",
    "       \n",
    "  # Counts of words\n",
    "  NC = list()\n",
    "  for i in range(0,m):\n",
    "    nc = [1,1]\n",
    "    for j in range(0,n):\n",
    "      nc[prevsample['l'][j]] += prevsample['data'][j][i]\n",
    "    NC.append(nc)\n",
    "    \n",
    "  for j in range(0,n):\n",
    "    # for each document label\n",
    "    if labels[j] == -1:\n",
    "      # If the label is not from the training set, let's resample it\n",
    "      \n",
    "      # subtract j's word counts from NC of whatever class it's member of\n",
    "      for i in range(0,m):\n",
    "        NC[i][prevsample['l'][j]] -= prevsample['data'][j][i]\n",
    "\t\n",
    "      # subtract 1 from the count of documents with label Lj\n",
    "      C[prevsample['l'][j]] -= 1\n",
    "      \n",
    "      if C[0] == 0:\n",
    "        p1 = 1\n",
    "      elif C[1] == 0:\n",
    "        p1 = 0\n",
    "      else:\n",
    "          # compute the product of probabilities (sum of logs)\n",
    "          value0 = log(float(C[0])/(n+1))\n",
    "          value1 = log(float(C[1])/(n+1))\n",
    "          for i in range(0,m):\n",
    "              value0 += log(prevsample['theta'][0][i])*prevsample['data'][j][i]\n",
    "              value1 += log(prevsample['theta'][1][i])*prevsample['data'][j][i]\n",
    "          # Divide by a common number to avoid problems of small numbers.\n",
    "          divisor = (value0+value1)/2\n",
    "          value0 = exp(value0-divisor)\n",
    "          value1 = exp(value1-divisor)\n",
    "          p1 = value1 / (value0 + value1)\n",
    "      \n",
    "      # sample the new label\n",
    "      prevsample['l'][j] = mtrand.binomial(1,p1)\n",
    "      \n",
    "      # add 1 to the counts of documents with label Lj\n",
    "      C[prevsample['l'][j]] += 1\n",
    "      \n",
    "      # add j's word counts to the total word counts for class Lj\n",
    "      for i in range(0,m):\n",
    "        NC[i][prevsample['l'][j]] += prevsample['data'][j][i]\n",
    "      \n",
    "  # sample new theta\n",
    "  t0 = list()\n",
    "  t1 = list()\n",
    "  for i in range(0,m):\n",
    "    # For each word\n",
    "    t0.append(NC[i][0])\n",
    "    t1.append(NC[i][1])\n",
    "  prevsample['theta'][0] = mtrand.dirichlet(t0)\n",
    "  prevsample['theta'][1] = mtrand.dirichlet(t1)\n",
    "  \n",
    "  return prevsample\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  import sys\n",
    "  from matplotlib import pyplot\n",
    "  import numpy\n",
    "  if len(sys.argv) != 4:\n",
    "    print(\"Usage: python {0} num_docs num_words num_iterations\".format(sys.argv[0]))\n",
    "    print(\"e.g. python {0} 50 5 5\".format(sys.argv[0]))\n",
    "    exit()\n",
    "    \n",
    "  n = int(sys.argv[1])\n",
    "  m = int(sys.argv[2])\n",
    "  iterations = int(sys.argv[3])\n",
    "  nsamples = 1000\n",
    "  number = 0\n",
    "  for p in range(0,10):\n",
    "    partition = float(p)/10\n",
    "    number += 1\n",
    "    accuracy = list()\n",
    "    for i in range(0,nsamples):\n",
    "      data = genNB(n,m)\n",
    "      accuracy.append(gibbsSampling(data['data'],data['l'],iterations,partition))\n",
    "    print(\"Accuracy average at {0} is {1}\".format(partition,numpy.average(accuracy)))\n",
    "#    print \"Accuracy values are\", accuracy\n",
    "    pyplot.subplot(2,5,number)\n",
    "    pyplot.hist(accuracy,bins=10)\n",
    "    pyplot.ylim(ymax=nsamples)\n",
    "    pyplot.xlim(xmin=0)\n",
    "    pyplot.text(0.2,int(nsamples*0.8),'mean = {0:3.3f}'.format(numpy.mean(accuracy)))\n",
    "    pyplot.title(\"{0} for training\".format(partition))\n",
    "  pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
