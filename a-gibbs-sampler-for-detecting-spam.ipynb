{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gibbs Sampler for Spam Detection\n",
    "\n",
    "In an [earlier post](https://nbviewer.jupyter.org/github/bobflagg/gibbs-sampling-for-the-uninitiated/blob/master/Gibbs-sampling-for-the-Uninitiated.ipynb) I gave a Python implementation of the Gibbs sampler for text classification described in the excellent tutorial paper\n",
    "[Gibbs Sampling for the Uninitiated](https://www.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf).  In this notebook, I'll show how to use that sampler to detect spam.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Spam-vs-Ham Training Corpus\n",
    "\n",
    "I'll use a combination of the [Enron-Spam](http://www.aueb.gr/users/ion/data/enron-spam/) data set and the [SpamAssassin public corpus](https://spamassassin.apache.org/publiccorpus/) to buld a training set for our spam detector. To simpify the presentation, I've done some minimal pre-processing of the original data and collected \n",
    "the results in a file in which each line contains the class (ham or spam) and the text of an \n",
    "e-mail message separated by a tab.  The corpus is in contained in the archive at data/spam-or-ham.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from numpy.random import beta, binomial, dirichlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(path='data/spam-or-ham.txt'):\n",
    "    fp = open(path, 'r')\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for line in fp:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            label, text = line.split('\\t')\n",
    "            labels.append(label)\n",
    "            texts.append(text)\n",
    "    fp.close()    \n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_vocabulary(texts, V, min_cnt=50, max_cnt=10000):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            counter[word.lower()] += 1    \n",
    "    words = [w for w in counter.keys() if counter[w] > min_cnt and counter[2] < max_cnt]\n",
    "    words = sorted(words, key=lambda x: counter[x])\n",
    "    return set(words[-V:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = 10000\n",
    "texts, labels = read_data()\n",
    "vocabulary = select_vocabulary(texts, V)\n",
    "word2id = {w:i for i, w in enumerate(vocabulary)}\n",
    "id2word = {i:w  for i, w in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_corpus(texts, vocabulary):\n",
    "    corpus = []\n",
    "    for text in texts:\n",
    "        words = [w.lower() for w in text.split() if w.lower() in vocabulary]\n",
    "        ids = [word2id[w] for w in words]\n",
    "        counter = Counter(ids)\n",
    "        document = {(i,c) for i, c in counter.items()}\n",
    "        corpus.append(document)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = build_corpus(texts, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_labels(J, gamma_pi):\n",
    "    pi = beta(gamma_pi[0], gamma_pi[1])\n",
    "    return binomial(1, pi, J)\n",
    "\n",
    "def initialize(W, labels, gamma_pi, gamma_theta):\n",
    "    N = len(W)\n",
    "    M = len(labels)\n",
    "    V = len(gamma_theta)\n",
    "\n",
    "    L = sample_labels(N - M, gamma_pi)\n",
    "    theta = dirichlet(gamma_theta, 2)\n",
    "\n",
    "    C = np.zeros((2,))\n",
    "    C += gamma_pi\n",
    "    cnts = np.zeros((2, V))\n",
    "    cnts += gamma_theta\n",
    "    \n",
    "    for d, l in zip(W, labels.tolist() + L.tolist()):\n",
    "        for i, c in d: cnts[l][i] += c\n",
    "        C[l] += 1\n",
    "\n",
    "    return {'C':C, 'N':cnts, 'L':L, 'theta':theta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update(state, X):\n",
    "    C = state['C']\n",
    "    N = state['N']\n",
    "    L = state['L']\n",
    "    theta = state['theta']\n",
    "    # Update the labels for all documents:\n",
    "    for j, l in enumerate(L):\n",
    "        # Drop document j from the corpus:\n",
    "        for i, c in X[j]: N[l][i] -= c\n",
    "        C[l] -= 1  \n",
    "        # Compute the conditional probability that L[j] = 1:  \n",
    "        if C[0] == 1: pi = 1.0\n",
    "        elif C[1] == 1 <= 0: pi = 0.0 \n",
    "        else:\n",
    "            # compute the product of probabilities (sum of logs)\n",
    "            d = np.sum(C) - 1\n",
    "            v0 = np.log((C[0] - 1.0) / d)\n",
    "            v1 = np.log((C[1] - 1.0) / d)\n",
    "            for i, c in X[j]:\n",
    "                v0 += c * np.log(theta[0,i])\n",
    "                v1 += c * np.log(theta[1,i])\n",
    "            m = max(v0, v1)\n",
    "            v0 = np.exp(v0 - m)\n",
    "            v1 = np.exp(v1 - m)\n",
    "            pi = v1 / (v0 + v1)\n",
    "        if np.isnan(pi):\n",
    "            d = np.sum(C) - 1\n",
    "            v0 = np.log((C[0] - 1.0) / d)\n",
    "            v1 = np.log((C[1] - 1.0) / d)\n",
    "            print('v0, v1',v0, v1)\n",
    "            for i, c in X[j]:\n",
    "                v0 += c * np.log(theta[0,i])\n",
    "                v1 += c * np.log(theta[1,i])\n",
    "                print('v0, v1', v0, v1)\n",
    "            m = max(v0, v1)\n",
    "            print('m', m, v0 - m, v1 - m)\n",
    "            v0 = np.exp(v0 - m)\n",
    "            v1 = np.exp(v1 - m)\n",
    "            print('v0, v1', v0, v1)\n",
    "            pi = v1 / (v0 + v1)\n",
    "        # Sample the new label from the conditional probability:\n",
    "        l = binomial(1, pi)\n",
    "        L[j] = l\n",
    "        # Add document j back into the corpus:\n",
    "        C[l] += 1\n",
    "        for i, c in X[j]: N[l][i] += c\n",
    "    #print('--->>>', np.min(cnts[0]), np.min(cnts[1]))\n",
    "    # Update the topics:\n",
    "    theta[0] = dirichlet(N[0])\n",
    "    theta[1] = dirichlet(N[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_sampler(W, labels, iterations, gamma_pi, gamma_theta):\n",
    "    state = initialize(W, labels, gamma_pi, gamma_theta)\n",
    "    X = W[len(labels):]\n",
    "    for t in range(iterations): update(state, X)\n",
    "    return state['L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(L_true, L_predicted):\n",
    "    correct = 0\n",
    "    for i, l in enumerate(L_predicted):\n",
    "        if L_true[i] == l: correct += 1\n",
    "    accuracy = float(correct)/len(L_predicted)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "gamma_pi = (1, 1)\n",
    "gamma_theta = [1] * V\n",
    "\n",
    "N = 10000\n",
    "W = corpus[:N]\n",
    "n = int(N * 0.9)\n",
    "labels_observed = np.array([0 if x == 'ham' else 1 for x in labels[:n]])\n",
    "labels_unobserved = np.array([0 if x == 'ham' else 1 for x in labels[n:N]])\n",
    "    \n",
    "iterations = 200\n",
    "L = run_sampler(W, labels_observed, iterations, gamma_pi, gamma_theta)\n",
    "accuracy = compute_accuracy(labels_unobserved, L)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "counts = count_vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "NEWLINE = '\\n'\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def build_data_frame(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for file_name, text in read_files(path):\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\n",
    "SOURCES = [\n",
    "    ('data/spamassassin/spam',        SPAM),\n",
    "    ('data/spamassassin/easy_ham',    HAM),\n",
    "    ('data/spamassassin/hard_ham',    HAM),\n",
    "    ('data/enron/beck-s',      HAM),\n",
    "    ('data/enron/BG',          SPAM),\n",
    "    ('data/enron/farmer-d',    HAM),\n",
    "    ('data/enron/GP',          SPAM),\n",
    "    ('data/enron/kaminski-v',  HAM),\n",
    "    ('data/enron/kitchen-l',   HAM),\n",
    "    ('data/enron/lokay-m',     HAM),\n",
    "    ('data/enron/SH',          SPAM),\n",
    "    ('data/enron/williams-w3', HAM)\n",
    "]\n",
    "\n",
    "data = DataFrame({'text': [], 'class': []})\n",
    "for path, classification in SOURCES:\n",
    "    data = data.append(build_data_frame(path, classification))\n",
    "\n",
    "data = data.reindex(np.random.permutation(data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/home/data/english-words/words.txt'\n",
    "fp = open(path, 'r')\n",
    "english_words = set()\n",
    "for word in fp:\n",
    "    word = word.strip()\n",
    "    if word: english_words.add(word)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string \n",
    "from nltk import word_tokenize\n",
    "english_words |= set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "WS_RE = re.compile(r'\\s\\s+')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def remove_white_extra_space(text):\n",
    "    text = text.strip()\n",
    "    return WS_RE.sub(' ', text)\n",
    "\n",
    "def tokenize_and_join(text):\n",
    "    words = word_tokenize(text)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55326\n"
     ]
    }
   ],
   "source": [
    "import codecs \n",
    "path = 'data/spam-or-ham.txt'\n",
    "fp = codecs.open(path, 'w', 'UTF-8')\n",
    "cnt = 0\n",
    "for label, message in zip(data['class'].tolist(), data['text'].tolist()):\n",
    "    text = remove_tags(message)\n",
    "    text = remove_white_extra_space(text)\n",
    "    text = tokenize_and_join(text)\n",
    "    if len(text) > 10: fp.write(\"%s\\t%s\\n\" % (label, text))\n",
    "    cnt += 1\n",
    "    #if cnt > 10: break\n",
    "fp.close()\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "6.1.1 Beyond SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'CPUC',\n",
       " 'still',\n",
       " 'plans',\n",
       " 'to',\n",
       " 'vote',\n",
       " 'on',\n",
       " 'suspending',\n",
       " 'DA',\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'but',\n",
       " 'a',\n",
       " 'few',\n",
       " 'new',\n",
       " 'twists',\n",
       " ':',\n",
       " 'The',\n",
       " 'State',\n",
       " 'Treasurer',\n",
       " 'announced',\n",
       " 'last',\n",
       " 'week',\n",
       " 'that',\n",
       " 'the',\n",
       " 'State',\n",
       " 'has',\n",
       " 'no',\n",
       " 'cash',\n",
       " 'flow',\n",
       " 'crisis',\n",
       " 'and',\n",
       " 'has',\n",
       " 'adequate',\n",
       " 'cash',\n",
       " 'reserves',\n",
       " 'and',\n",
       " 'short-term',\n",
       " 'borrowing',\n",
       " 'capacity',\n",
       " 'to',\n",
       " 'last',\n",
       " 'until',\n",
       " 'the',\n",
       " 'end',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fascal',\n",
       " 'year',\n",
       " 'in',\n",
       " '2002',\n",
       " '(',\n",
       " 'we',\n",
       " 'think',\n",
       " 'July',\n",
       " 'ends',\n",
       " 'the',\n",
       " 'fiscal',\n",
       " 'year',\n",
       " ')',\n",
       " 'President',\n",
       " 'of',\n",
       " 'Senate',\n",
       " 'says',\n",
       " 'at',\n",
       " 'press',\n",
       " 'conference',\n",
       " 'on',\n",
       " 'Monday',\n",
       " 'that',\n",
       " 'he',\n",
       " 'intends',\n",
       " 'to',\n",
       " 'ask',\n",
       " 'CPUC',\n",
       " 'to',\n",
       " 'study',\n",
       " 'how',\n",
       " 'to',\n",
       " 'provide',\n",
       " 'DA',\n",
       " 'while',\n",
       " 'still',\n",
       " 'protecting',\n",
       " 'small',\n",
       " 'customers',\n",
       " '(',\n",
       " 'letter',\n",
       " 'not',\n",
       " 'yet',\n",
       " 'written',\n",
       " ')',\n",
       " 'Enron',\n",
       " \"'s\",\n",
       " 'coalitions',\n",
       " ',',\n",
       " 'AReM',\n",
       " 'and',\n",
       " 'WPTF',\n",
       " ',',\n",
       " 'filed',\n",
       " 'a',\n",
       " 'motion',\n",
       " 'at',\n",
       " 'the',\n",
       " 'CPUC',\n",
       " 'today',\n",
       " 'saying',\n",
       " '.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''The CPUC still plans to vote on suspending DA on Thursday but a few new twists: The State Treasurer announced last week that the State has no cash flow crisis and has adequate cash reserves and short-term borrowing capacity to last until the end of the fascal year in 2002 (we think July ends the fiscal year) President of Senate says at press conference on Monday that he intends to ask CPUC to study how to provide DA while still protecting small customers (letter not yet written) Enron's coalitions, AReM and WPTF, filed a motion at the CPUC today saying.'''\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "counts = count_vectorizer.fit_transform(data['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "targets = data['class'].values\n",
    "classifier.fit(counts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam', 'ham'], \n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = ['Free Viagra call today!', \"I'm going to attend the Linux users group tomorrow.\"]\n",
    "example_counts = count_vectorizer.transform(examples)\n",
    "predictions = classifier.predict(example_counts)\n",
    "predictions # [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam', 'ham'], \n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',  CountVectorizer()),\n",
    "    ('classifier',  MultinomialNB()) ])\n",
    "\n",
    "pipeline.fit(data['text'].values, data['class'].values)\n",
    "pipeline.predict(examples) # ['spam', 'ham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birksworks/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails classified: 55326\n",
      "Score: 0.942656880273\n",
      "Confusion matrix:\n",
      "[[21658   180]\n",
      " [ 3472 30016]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "k_fold = KFold(n=len(data), n_folds=6)\n",
    "scores = []\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = data.iloc[train_indices]['text'].values\n",
    "    train_y = data.iloc[train_indices]['class'].values\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text'].values\n",
    "    test_y = data.iloc[test_indices]['class'].values\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Total emails classified:', len(data))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
