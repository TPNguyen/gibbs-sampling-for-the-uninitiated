{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gibbs Sampler for Spam Detection\n",
    "\n",
    "In an [earlier post](https://nbviewer.jupyter.org/github/bobflagg/gibbs-sampling-for-the-uninitiated/blob/master/Gibbs-sampling-for-the-Uninitiated.ipynb) I gave a Python implementation of the Gibbs sampler for text classification described in the excellent tutorial paper\n",
    "[Gibbs Sampling for the Uninitiated](https://www.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf).  In this notebook, I'll show how to use that sampler to detect spam.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Spam-vs-Ham Training Corpus\n",
    "\n",
    "I'll use a combination of the [Enron-Spam](http://www.aueb.gr/users/ion/data/enron-spam/) data set and the [SpamAssassin public corpus](https://spamassassin.apache.org/publiccorpus/) to buld a training set for our spam detector. To simpify the presentation, I've done some minimal pre-processing of the original data and collected \n",
    "the results in a file in which each line contains the class (ham or spam) and the text of an \n",
    "e-mail message separated by a tab.  The corpus is available in the data directory of the github repository [Gibbs Sampling for the Uninitiated](https://github.com/bobflagg/gibbs-sampling-for-the-uninitiated).  Once the archive containing the data is expanded, the text and labels can be loaded with the following method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(path='data/spam-or-ham.txt'):\n",
    "    fp = open(path, 'r')\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for line in fp:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            label, text = line.split('\\t')\n",
    "            labels.append(label)\n",
    "            texts.append(text)\n",
    "    fp.close()    \n",
    "    return texts, labels\n",
    "texts, labels = read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 0 [ham]:\n",
      "Louise - OEC will come along with EECC & NEPCO . Brian Stanley and Keith Dodson will run that group . Speak with you soon . Hope you are feeling well . Regards - Dan Louise Kitchen @ ECT 03/14/2001 12:18 PM To : Dan Leff/HOU/EES @ EES cc : Subject : OEC What happens to the OEC group - does that currently report to you and will it going forward or does that move too ?\n",
      "\n",
      "Message 1 [spam]:\n",
      "-- -- 1305094201867723127 Content-Type : text/plain ; charset= '' iso-6434-4 '' Content-Transfer-Encoding : quoted-printable Content-Description : continued ross terramycin Hi , Make your ordinary 56k modem go speeds of upto 250k+ ! ( Average increased speeds of 195 - 200K ! ) Download music/programs in seconds , not minutes ! This hardware is compatible with EVERY Dialup ISP ! Check it out : http : //click4point.com/ Do n't forget to save our site URL ! ( If its not up , try again later ! ) Goodbye , http : //click4point.com/r/ Jenna=20 -- -- 1305094201867723127 --\n",
      "\n",
      "Message 2 [ham]:\n",
      "Attached is a draft press release regarding the partnership between Enron and Applied Terravision . The plan is to release this on Monday , March 19 or at the latest early on Tuesday , March 20 . If you have any comments , please forward them to me at your earliest convenience . Thanks . Eric\n",
      "\n",
      "Message 3 [spam]:\n",
      "never get a traffic ticket again ! -ia 55 m Have you been caught by a red light camera yet ? if yes , then you have already paid $ 100 ? $ 150 ? $ 250 ? or MORE for EACH offense ! What if I told you there is a way to AVOID these fines forever ? 100 % Gauranteed http : //= xxxxsupersite.com/index.php ? id=3D173 & affid=3D4586 Come Here to find out how http : //= xxxxsupersite.com/index.php ? id=3D173 & affid=3D4586 Check here if you would not like to receive future mailings . http : //xxxxsupersite.com/gone.= php persona certiorari caloric mighty sanction alga breast corpse waterloo = waterbury bugeyed dreamy farcical pediatric revile subliminal defensible = dilute pack condescend inward publish phenomena albatross cometary = magnanimity metronome columbus felicia scratch gainesville epiphysis = gadgetry davies fixture shuffleboard iceland soapstone collage = consumptive danish native stencil mel nina chloroform glut bigot = snowmobile margo ambulatory cygnus viscoelastic pounce brandt outlawry = office wistful daze spheroidal airlock hatteras laredo data lucille = edwardian discuss elementary delectate finland headstand angie thump = charles debugged index canton henequen only craze clog passport = animadversion aniseikonic thud anguish catherine barnabas seminar cb = abramson serine renewal louse poultry audience fourth timeshare = starlight cuttlebone aviatrix defocus duplicable account = anthropomorphism crystal cuckoo eruption lifeblood gin cram decrypt = consular capitoline cicero asuncion trumpery cyrillic cohere dave embed = france divide goldstine babyhood ifni psychopath barcelona reception = accession darry purr muscular barbados degassing blithe brunt comma = avoid catastrophe rut interpret chairwomen novelty roundhouse imbrium = ripley betty changeover pow bootlegged accompany annulus cocklebur clock = exportation leo laminate stadium arty conjuncture debra extramarital = indefinite malleable balinese aggressive linkage cereus kenya bricklay = glycine improvisation oases irrecoverable sicklewort meantime adrift dey = tolstoy chauncey wed empathy delay klaus delia depredate graze corp = sophisticate medial pterodactyl flagrant usia coeditor tetrafluoride = cross douse acknowledge bequest deuteron oases dorado deem ribbon = capacious calamus squirt octavia rankine amphibious basel malcolm extend = allied account duplicate glint clogging donaldson malice drab brandon = breastwork ping defrost sandpaper feel expense hawley caine emaciate = dirge arousal diverse cornea hermann chalcedony apr medicate impede = sparling tick afternoon indigestible escadrille halocarbon beady = decisional befit colloq consonant confiscable featherbedding kidnapping = divert bookseller brochure exquisite torrent abner annual apocalyptic = assyria eventide chevalier starlet teahouse cradle ski brothel enter pa = innuendo\n",
      "\n",
      "Message 4 [ham]:\n",
      "Can we verified this gas flowed ? If it did it should have been billed on his term deal if there was one in place . If the gas flowed and we did not invoice for it on term let me know and I will put it in . Janet H Wallis 03/22/2001 04:53 PM To : Gary W Lamphier/HOU/ECT @ ECT cc : Katherine Herrera/Corp/Enron @ ENRON Subject : CP & L Bob says he was not billed for a purchase he made from you at 5K $ 5.16 on Feb 21st . Will you check this out and get with Bob A and Katherine Herrera . JW\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): \n",
    "    print(\"Message %d [%s]:\\n%s\\n\" % (i, labels[i], texts[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select a vocabulary, I'll restrict to the 10,000 most frequently occurring lower case words in the corpus afer \n",
    "removing words that appear too frequently to be informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def select_vocabulary(texts, V, max_cnt=10000):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            counter[word.lower()] += 1    \n",
    "    words = [w for w in counter.keys() if counter[w] < max_cnt]\n",
    "    words = sorted(words, key=lambda x: counter[x])\n",
    "    return set(words[-V:])\n",
    "\n",
    "V = 10000\n",
    "vocabulary = select_vocabulary(texts, V)\n",
    "word2id = {w:i for i, w in enumerate(vocabulary)}\n",
    "id2word = {i:w  for i, w in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now I'll build a corpus that can be used in our sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_corpus(texts, vocabulary):\n",
    "    corpus = []\n",
    "    for text in texts:\n",
    "        words = [w.lower() for w in text.split() if w.lower() in vocabulary]\n",
    "        ids = [word2id[w] for w in words]\n",
    "        counter = Counter(ids)\n",
    "        document = {(i,c) for i, c in counter.items()}\n",
    "        corpus.append(document)\n",
    "    return corpus\n",
    "\n",
    "corpus = build_corpus(texts, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Spam-or-Ham with Gibbs Sampling\n",
    "\n",
    "The next few cells are taken from the \n",
    "[earlier post](https://nbviewer.jupyter.org/github/bobflagg/gibbs-sampling-for-the-uninitiated/blob/master/Gibbs-sampling-for-the-Uninitiated.ipynb)\n",
    "and define the sampler and a utility method to evaluate results of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_labels(J, gamma_pi):\n",
    "    pi = beta(gamma_pi[0], gamma_pi[1])\n",
    "    return binomial(1, pi, J)\n",
    "\n",
    "def initialize(W, labels, gamma_pi, gamma_theta):\n",
    "    N = len(W)\n",
    "    M = len(labels)\n",
    "    V = len(gamma_theta)\n",
    "\n",
    "    L = sample_labels(N - M, gamma_pi)\n",
    "    theta = dirichlet(gamma_theta, 2)\n",
    "\n",
    "    C = np.zeros((2,))\n",
    "    C += gamma_pi\n",
    "    cnts = np.zeros((2, V))\n",
    "    cnts += gamma_theta\n",
    "    \n",
    "    for d, l in zip(W, labels.tolist() + L.tolist()):\n",
    "        for i, c in d: cnts[l][i] += c\n",
    "        C[l] += 1\n",
    "\n",
    "    return {'C':C, 'N':cnts, 'L':L, 'theta':theta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update(state, X):\n",
    "    C = state['C']\n",
    "    N = state['N']\n",
    "    L = state['L']\n",
    "    theta = state['theta']\n",
    "    # Update the labels for all documents:\n",
    "    for j, l in enumerate(L):\n",
    "        # Drop document j from the corpus:\n",
    "        for i, c in X[j]: N[l][i] -= c\n",
    "        C[l] -= 1  \n",
    "        # Compute the conditional probability that L[j] = 1:  \n",
    "        if C[0] == 1: pi = 1.0\n",
    "        elif C[1] == 1 <= 0: pi = 0.0 \n",
    "        else:\n",
    "            # compute the product of probabilities (sum of logs)\n",
    "            d = np.sum(C) - 1\n",
    "            v0 = np.log((C[0] - 1.0) / d)\n",
    "            v1 = np.log((C[1] - 1.0) / d)\n",
    "            for i, c in X[j]:\n",
    "                v0 += c * np.log(theta[0,i])\n",
    "                v1 += c * np.log(theta[1,i])\n",
    "            m = max(v0, v1)\n",
    "            v0 = np.exp(v0 - m)\n",
    "            v1 = np.exp(v1 - m)\n",
    "            pi = v1 / (v0 + v1)\n",
    "        # Sample the new label from the conditional probability:\n",
    "        l = binomial(1, pi)\n",
    "        L[j] = l\n",
    "        # Add document j back into the corpus:\n",
    "        C[l] += 1\n",
    "        for i, c in X[j]: N[l][i] += c\n",
    "    # Update the topics:\n",
    "    theta[0] = dirichlet(N[0])\n",
    "    theta[1] = dirichlet(N[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_sampler(W, labels, iterations, gamma_pi, gamma_theta):\n",
    "    state = initialize(W, labels, gamma_pi, gamma_theta)\n",
    "    X = W[len(labels):]\n",
    "    for t in range(iterations): update(state, X)\n",
    "    return state['L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(L_true, L_predicted):\n",
    "    correct = 0\n",
    "    for i, l in enumerate(L_predicted):\n",
    "        if L_true[i] == l: correct += 1\n",
    "    accuracy = float(correct)/len(L_predicted)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll run the sampler on the first 10,000 records in the corpus, training on 8,000 and testing on the remaining 2,000. Since the Gibbs sampler implementation has not yet been optimized, this takes about a minute on my machine.  Feel free to lower the value of N and the number of iterations when calling the following method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_spam_or_ham(N, p, iterations=100):\n",
    "    gamma_pi = (1, 1)\n",
    "    gamma_theta = [1] * V\n",
    "\n",
    "    W = corpus[:N]\n",
    "    n = int(N * p)\n",
    "    labels_observed = np.array([0 if x == 'ham' else 1 for x in labels[:n]])\n",
    "    labels_unobserved = np.array([0 if x == 'ham' else 1 for x in labels[n:N]])\n",
    "\n",
    "    \n",
    "    L = run_sampler(W, labels_observed, iterations, gamma_pi, gamma_theta)\n",
    "    accuracy = compute_accuracy(labels_unobserved, L)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.4 s, sys: 16 ms, total: 53.4 s\n",
      "Wall time: 53.5 s\n",
      "0.943\n"
     ]
    }
   ],
   "source": [
    "%time accuracy = predict_spam_or_ham(N=10000, p=0.8, iterations=100)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives an accuracy of about 94%, which we could certainly improve by a more careful selection of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam-or-Ham with scikit-learn\n",
    "\n",
    "As a sanity check on the work above, I'll train and evaluate a Naive Bayes classifier on our data, using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.44 s, sys: 0 ns, total: 1.44 s\n",
      "Wall time: 1.45 s\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "N = 10000\n",
    "#N = len(texts)\n",
    "p = 0.8\n",
    "n = int(N * p)\n",
    "X_train = texts[:n]\n",
    "Y_train = labels[:n]\n",
    "X_test = texts[n:N]\n",
    "Y_test = labels[n:N]\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',  CountVectorizer(vocabulary=vocabulary)),\n",
    "    ('classifier',  MultinomialNB()) \n",
    "])\n",
    "%time pipeline.fit(X_train, Y_train)\n",
    "Y_predict = pipeline.predict(X_test)\n",
    "accuracy = compute_accuracy(Y_test, Y_predict)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This gives an accuracy of about 96%, which is definitely better than what I was able to achieve with the Gibbs sampler\n",
    "but still in the same ball park."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
